## 上传文档与上下文处理机制说明

### 一、上传文档是怎么被用到的？

- **1. 文档入库阶段**
  - 文档上传后会被拆分成多个「片段」（chunk），每个片段保存两类信息：
    - **原文文本**：后面真正喂给大模型看的内容。
    - **向量表示（Embedding）**：用句向量模型（如 `sentence-transformers/all-MiniLM-L6-v2`）生成，用于相似度检索。
  - 这些信息会被存入数据库 / 向量索引中。

- **2. 提问阶段（RAG 流程）**
  - 对用户问题也做一次 **Embedding**。
  - 用向量检索在所有片段中找到若干个「最相关片段」。
  - **真正发给大模型的是：问题 + 若干片段的原文文本**，而不是向量本身。
  - 向量只负责「从大量文档里先筛出几个最相关的片段」。

> 结论：不论是单次上传文档提问，还是长期知识库查询，**大模型看到的始终是原文文本，上下文的选择依靠向量检索完成**。

---

### 二、为什么要关注「分块数」而不是文档总大小？

- **显存压力主要来自上下文 KV Cache**
  - 文档多大并不直接决定显存占用，关键在于「这次提问时送进大模型的 token 数量」。
  - 每多 1k token 上下文，就要多一块 KV Cache；在大模型（如 DeepSeek 70B int4）上尤其明显。

- **真正关键指标**
  - **每个片段的长度（chunk size / tokens per chunk）**。
  - **每次检索后使用的片段数量（Top K / Max Chunks）**。
  - 只要控制好「**每次问答引用多少片段 + 总 token 数**」，就可以在不限制用户上传大文档的前提下，避免显存/内存爆掉。

---

### 三、在本项目上的推荐设置（以 DeepSeek 70B int4 + 2×24G 为例）

- **分块粒度（chunk size）**
  - 建议控制在 **512–1024 tokens / 片段**（大约 400–800 字级别）。

- **单次检索返回的片段数量**
  - 推荐将 `Top K` / `Max Chunks` 设置为 **4–8**：
    - 足够覆盖主要语义；
    - 又能把总上下文 token 控制在几千以内，显存压力可控。

- **文档大小建议**
  - 单文档大小可以相对宽松（10–20MB 或 < 20 万字）。
  - 系统只会在当前问题中选出前 K 个最相关的片段送入上下文，而不会把整篇文档一次性塞给模型。

> 实践策略：**优先限制「每次检索取多少块」和「总上下文 token 数」，而不是限制用户能上传多大的文档。**

---

### 四、配置建议（思路）

- 在「检索 / 知识库」相关设置中：
  - 找到并调整类似 `Top K`、`Max Chunks`、`Results per query` 的参数为 **4–8**。
  - 如有 `Max input tokens` / `Max context tokens` 配置，可设置为模型上下文长度的一半左右，构造 prompt 时按 token 数截断。

- 调优流程：
  1. 先用较小值（例如 Top K = 4）运行，观察显存占用和回答质量。
  2. 如需更全面的上下文，再逐步增加 Top K / chunk size，同时留意推理稳定性。


